<!DOCTYPE html>
<html>

    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/jekyll32/css/main.css">
    <title>《非线性最优化基础》学习笔记</title>
  </head>


  <body>

    

    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">《非线性最优化基础》学习笔记</h1>
    <p class="post-meta"><time datetime="2015-08-02T10:33:54-04:00" itemprop="datePublished">Aug 2, 2015</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>《<a href="http://book.douban.com/subject/6510671/">非线性最优化基础</a>》 作者 <a href="http://www.seto.nanzan-u.ac.jp/~fuku/index.html">福嶋雅夫</a> (《非线性最优化基础》（豆瓣链接：<a href="http://book.douban.com/subject/6510671/">http://book.douban.com/subject/6510671/</a>）。福嶋雅夫（Masao Fukushima），教授，日本南山大学理工学院系统与数学科学系，日本京都大学名誉教授，加拿大滑铁卢大学/比利时那慕尔大学/澳大利亚新南威尔士大学客座教授。主页：<a href="http://www.seto.nanzan-u.ac.jp/~fuku/index.html">http://www.seto.nanzan-u.ac.jp/~fuku/index.html</a>。)</p>

<p>该文为<a href="http://web.xidian.edu.cn/xcfeng/">冯象初教授</a>(冯象初，教授，西安电子科技大学数学系。主页：<a href="http://web.xidian.edu.cn/xcfeng/">http://web.xidian.edu.cn/xcfeng/</a>)有关非线性最优化的讲座的笔记。</p>

<h2 id="section">主要内容</h2>

<p><strong>理论基础</strong></p>

<ol>
  <li>凸函数、闭函数</li>
  <li>共轭函数</li>
  <li>鞍点问题</li>
  <li>Lagrange 对偶问题</li>
  <li>Lagrange 对偶性的推广</li>
  <li>Fenchel 对偶性</li>
</ol>

<p><strong>算法</strong></p>

<ol>
  <li>Proximal gradient methods</li>
  <li>Dual proximal gradient methods</li>
  <li>Fast proximal gradient methods</li>
  <li>Fast dual proximal gradient methods</li>
</ol>

<!--more-->

<h2 id="section-1">理论基础</h2>

<h3 id="section-2">凸函数、闭函数</h3>

<p>给定函数 \(f : \Re^n \to [-\infty, +\infty] \)，称 \(\Re^{n+1}\) 的子集</p>

<p>\[
graph \; f = \left\{ (\mathbf{x}, \beta)^T \in \Re^{n+1} \mid \beta = f(\mathbf{x}) \right\} ,
\]</p>

<p>为 \(f\) 的<strong>图像</strong>（graph），而称位于 \(f\) 的图像上方的点的全体构成的集合</p>

<p>\[
epi \; f =\lbrace{} (\mathbf{x}, \beta)^T \in \Re^{n+1} \mid \beta \geqslant f(\mathbf{x}) \rbrace{}
\]</p>

<p>为 \(f\) 的<strong>上图</strong>（epigraph）。若上图 \(epi \; f\) 为凸集，则称 \(f\) 为<strong>凸函数</strong>(convex function)。</p>

<p><strong>定理 2.27</strong> 设 \( \mathcal{I} \) 为任意非空指标集，而 \(f_i : \Re^n \to [-\infty, +\infty] \; (i \in \mathcal{I})\) 均为凸函数，则由</p>

<p>\[
f(\mathbf{x}) = \sup \lbrace{} f_i(\mathbf{x}) \mid i \in \mathcal{I} \rbrace{}
\]</p>

<p>定义的函数 \(f : \Re^n \to [-\infty, +\infty] \) 为凸函数。进一步，若 \(\mathcal{I}\) 为有限指标集，每个 \(f_i\) 均为正常的凸函数，并且 \(\cap_{i \in \mathcal{I}} \; dom \; f_i \neq \varnothing \)，则 \(f\) 为正常凸函数。</p>

<p>若对任意收敛于 \(\mathbf{x}\) 的点列 \(\lbrace{} \mathbf{x}^k\rbrace{} \subseteq \Re^n\) 均有</p>

<p>\[ f(\mathbf{x}) \geqslant \limsup_{k \to \infty}f(\mathbf{x}^k) \]</p>

<p>成立，则称函数 \(f:\Re^n\to[-\infty,+\infty]\) 在 \(\mathbf{x}\) 处<strong>上半连续</strong>（upper semicontinuous）；反之，当</p>

<p>\[ f(\mathbf{x}) \leqslant \liminf_{k \to \infty}f(\mathbf{x}^k) \]</p>

<p>成立时，称 \(f\) 在 \(\mathbf{x}\) 处<strong>下半连续</strong>（lower semicontinuous）。若 \(f\) 在 \(\mathbf{x}\) 处既为上半连续又为下半连续，则称 \(f\) 在 \(\mathbf{x}\) 处<strong>连续</strong>（continuous）。</p>

<h3 id="section-3">共轭函数</h3>

<p>给定正常凸函数 \(f:\Re^n \to (-\infty,+\infty]\)，由</p>

<p>\[f^\ast(\mathbf{\xi}) = \sup \lbrace{} &lt;\mathbf{x},\mathbf{\xi}&gt;-f(\mathbf{x}) \mid \mathbf{x}\in \Re^n \rbrace{} \]</p>

<p>定义的函数 \(f^\ast:\Re^n \to [-\infty,+\infty]\) 称为 \(f\) 的<strong>共轭函数</strong>（conjuagate function）。</p>

<p><strong>定理 2.36</strong> 正常凸函数 \(f:\Re^n \to (-\infty,+\infty]\) 的共轭函数 \(f^\ast\) 必为闭正常凸函数。</p>

<h3 id="section-4">鞍点问题</h3>

<p>设 \(Y\) 与 \(Z\) 分别为 \(\Re^n\) 与 \(\Re^m\) 的非空子集，给定以 \(Y\times Z\) 为定义域的函数 \(K:Y\times Z\to[-\infty,+\infty]\)，定义两个函数 \(\eta:Y\to[-\infty,+\infty]\) 与 \(\zeta:Z\to[-\infty,+\infty]\) 如下：</p>

<p>\[\eta(\mathbf{y})=\sup\lbrace{} K(\mathbf{y},\mathbf{z}) \mid \mathbf{z} \in Z\rbrace{} \]</p>

<p>\[\zeta(\mathbf{z})=\inf\lbrace{} K(\mathbf{y},\mathbf{z}) \mid \mathbf{y} \in Y\rbrace{} \]</p>

<p>\[\min \; \; \eta(\mathbf{y})\]
\[s.t. \; \; \; \mathbf{y} \in Y\]</p>

<p>\[\max \; \; \zeta(\mathbf{z})\]
\[s.t. \; \; \; \mathbf{z} \in Z\]</p>

<p><strong>引理 4.1</strong> 对任意 \(\mathbf{y}\in Y\) 与 \(\mathbf{z}\in Z\) 均有 \(\zeta(\mathbf{z}) \leqslant \eta(\mathbf{y})\) 成立。进一步，还有
\[\sup\lbrace{} \zeta(\mathbf{z})\mid \mathbf{z}\in Z\rbrace{} \leqslant \inf\lbrace{} \eta(\mathbf{y})\mid \mathbf{y} \in Y\rbrace{} \]</p>

<p><strong>定理 4.1</strong> 点 \((\overline{\mathbf{y}},\overline{\mathbf{z}})\in Y\times Z\) 为函数 \(K:Y\times Z\to[-\infty,+\infty]\) 的鞍点的充要条件是 \(\overline{\mathbf{y}}\in Y\) 与 \(\overline{\mathbf{z}}\in Z\) 满足</p>

<p>\[\eta(\overline{\mathbf{y}})=\inf\lbrace{} \eta(\mathbf{y})\mid \mathbf{y}\in Y\rbrace{} =\sup\lbrace{} \zeta(\mathbf{z})\mid \mathbf{z}\in Z\rbrace{} =\zeta(\overline{\mathbf{z}})\]</p>

<h3 id="lagrange-">Lagrange 对偶问题</h3>

<p>考虑如下非线性规划问题：</p>

<p>\[ \min \; \; f(\mathbf{x}) \ s.t. \; \; g_i(\mathbf{x}) \leqslant 0, \; \; i=1, \cdots, m\]</p>

<p>其中 \(f: \Re^n \to \Re\), \(g_i: \Re^n \to \Re (i=1, \cdots, m)\)。</p>

<p>\[ S = \lbrace{} x \in \Re^n \mid g_i(\mathbf{x}) \leqslant 0 \text{, } \; \; i=1, \cdots, m \rbrace{}\]</p>

<p>\[ L_0(\mathbf{x}, \mathbf{\lambda}) = \begin{cases}
        f(\mathbf{x}) + \sum^m_{i=1}\lambda_ig_i(\mathbf{x})\;, &amp; \mathbf{\lambda} \geqslant \mathbf{0} <br />
        -\infty \; , &amp; \mathbf{\lambda} \ngeqslant \mathbf{0}
    \end{cases}
\]</p>

<p>\[ \theta(\mathbf{x}) = f(\mathbf{x}) + \delta_S(\mathbf{x})\]</p>

<p>\[ \theta(\mathbf{x}) = \sup \lbrace{} L_0(\mathbf{x}, \mathbf{\lambda}) \mid \mathbf{\lambda} \in \Re^m \rbrace{}\]</p>

<p>\[ \omega_0(\mathbf{\lambda}) = \inf \lbrace{} L_0(\mathbf{x}, \mathbf{\lambda}) \mid \mathbf{x} \in \Re^n \rbrace{} \]</p>

<p>Constrains relax</p>

<p>\[ F_0(\mathbf{x}, \mathbf{u}) = \begin{cases}
        f(\mathbf{x}),  &amp; \mathbf{x} \in        S(\mathbf{u}) &amp; \min  &amp; f(\mathbf{x}) &amp; &amp; <br />
        +\infty,      &amp; \mathbf{x} \notin S(\mathbf{u}) &amp; s.t.      &amp; g_i(\mathbf{x}) &amp; \leqslant u_i, &amp; i = 1, \cdots, m 
    \end{cases}
\]</p>

<p>\[ S(\mathbf{u}) = \lbrace{} \mathbf{x} \in \Re^n \mid g_i(\mathbf{x}) \leqslant u_i, \; i=1, \cdots, m \rbrace{} \]</p>

<p><strong>引理 4.5</strong> Lagrange 函数 \(L_0: \Re^{n+m} \to [-\infty, +\infty) \) 与函数 \(F_0: \Re^{n+m} \to (-\infty,+\infty]\) 之间有如下关系成立：</p>

<p>\[L_0(\mathbf{x}, \mathbf{\lambda}) = \inf \lbrace{} F_0(\mathbf{x}, \mathbf{u}) + &lt;\mathbf{\lambda}, \mathbf{u}&gt; \mid \mathbf{u} \in \Re^m \rbrace{}\]</p>

<p>\[F_0(\mathbf{x}, \mathbf{u}) = \sup \lbrace{} L_0(\mathbf{x}, \mathbf{\lambda}) - &lt;\mathbf{\lambda}, \mathbf{u}&gt; \mid \mathbf{\lambda} \in \Re^m \rbrace{}\]</p>

<h3 id="lagrange--1">Lagrange 对偶性的推广</h3>

<p>对于原始问题 \((P)\)，考虑函数 \(F: \Re^{n+M} \to (-\infty, +\infty]\)，使得对任意固定的 \(\mathbf{x} \in \Re^n\)，\(F(\mathbf{x}, \cdot): \Re^M \to (-\infty, +\infty]\) 均为闭正常凸函数，并且满足</p>

<p>\[ F(\mathbf{x}, \mathbf{0}) = \theta(\mathbf{x}) \text{, } \mathbf{x} \in \Re^n \]</p>

<p><strong>例 4.7</strong> 设 \(M = m\)，考虑函数 \(F_0: \Re^{n+m} \to (-\infty, +\infty]\)，利用满足 \(q(\mathbf{0}) = 0\) 的闭正常凸函数 \(q: \Re^m \to (-\infty, +\infty]\) 定义函数 \(F: \Re^{n+m} \to (-\infty, +\infty]\) 如下：</p>

<p>\[ F(\mathbf{x}, \mathbf{u}) = F_0(\mathbf{x}, \mathbf{u}) + q(\mathbf{u}) \]</p>

<p>\[ \theta(\mathbf{x}) = f(\mathbf{x}) + \delta_S(\mathbf{x}) \]
\[ \implies F(\mathbf{x}, \mathbf{u}) \mid F(\mathbf{x}, \mathbf{0}) = \theta(\mathbf{x}) \]
\[ \implies L(\mathbf{x}, \mathbf{\lambda}) = \inf \lbrace{} F(\mathbf{x}, \mathbf{u}) + &lt;\mathbf{\lambda}, \mathbf{u}&gt; \mid \mathbf{u} \in \Re^M \rbrace{} \]
\[ \implies \omega(\mathbf{\lambda}) = \inf \lbrace{} L(\mathbf{x}, \mathbf{\lambda}) \mid \mathbf{x} \in \Re^n \rbrace{} \]</p>

<h3 id="fenchel-">Fenchel 对偶性</h3>

<p>\[ \min_\mathbf{x} f(\mathbf{x}) + g(\mathbf{Ax}) \]</p>

<p>\[ \begin{cases} &amp; F(\mathbf{x}, \mathbf{0}) = \theta(\mathbf{x}), &amp; x \in \Re^n <br />
&amp; \theta(\mathbf{x}) = f(\mathbf{x}) + g(\mathbf{Ax}) &amp; \end{cases} \]</p>

<p>\[ \implies F(\mathbf{x}, \mathbf{u}) = f(\mathbf{x}) + g(\mathbf{Ax} + \mathbf{u}) \]
\[ \begin{eqnarray<em>} \implies L(\mathbf{x}, \mathbf{\lambda}) &amp; = &amp; \inf \lbrace{} f(\mathbf{x}) + g(\mathbf{Ax} + \mathbf{u}) + &lt;\mathbf{\lambda}, \mathbf{u}&gt; \mid \mathbf{u} \in \Re^m \rbrace{} <br />
&amp; = &amp; f(\mathbf{x}) - g^\ast(-\mathbf{\lambda}) - &lt;\mathbf{\lambda}, \mathbf{Ax}&gt; \end{eqnarray</em>}\]
\[ \begin{eqnarray<em>} \implies \omega(\mathbf{\lambda}) &amp; = &amp; \inf \lbrace{} f(\mathbf{x} - g^\ast(-\mathbf{\lambda}) - &lt;\mathbf{\lambda}, \mathbf{Ax}&gt; \mid \mathbf{x} \in \Re^n \rbrace{} <br />
&amp; = &amp; -f^\ast(\mathbf{A}^T\mathbf{\lambda}) - g^\ast(-\mathbf{\lambda}) \end{eqnarray</em>}\]</p>

<p>\[ \min_\mathbf{\lambda} f^\ast( \mathbf{A}^T\mathbf{\lambda} ) + g^\ast(-\mathbf{\lambda})\]
\[ \max_\mathbf{\lambda} -f^\ast(\mathbf{A}^T\mathbf{\lambda} ) - g^\ast(-\mathbf{\lambda})\]</p>

<h2 id="section-5">算法</h2>

<h3 id="proximal-gradient-method">1. Proximal Gradient Method</h3>

<p>参考 <a href="http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf">Algorithms for large-scale convex optimization - DTU 2010</a>(A Lecture note from "02930 Algorithms for Large-Scale Convex Optimization" taught by Per Christian Hansen (pch@imm.dtu.dk) and Professor Lieven Vandenberghe (<a href="http://www.seas.ucla.edu/~vandenbe/">http://www.seas.ucla.edu/~vandenbe/</a>) at Danmarks Tekniske Universitet (<a href="http://www.kurser.dtu.dk/2010-2011/02930.aspx?menulanguage=en-GB">http://www.kurser.dtu.dk/2010-2011/02930.aspx?menulanguage=en-GB</a>). The Download Link is found at the page of "EE227BT: Convex Optimization - Fall 2013" taught by Laurent El Ghaoui at Berkeley (<a href="http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf">http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf</a>). And both of the lectures mentioned the book "Convex Optimization" by Stephen Boyd and Lieven Vandenberghe (<a href="http://stanford.edu/~boyd/cvxbook/">http://stanford.edu/~boyd/cvxbook/</a>) and the software "CVX" - a MATLAB software for desciplined Convex Programming (<a href="http://cvxr.com/cvx/">http://cvxr.com/cvx/</a>). A similar lecture note on Proximal Gradient Method from "EE236C - Optimization Methods for Large-Scale Systems (Spring 2013-14)" (<a href="http://www.seas.ucla.edu/~vandenbe/ee236c.html">http://www.seas.ucla.edu/~vandenbe/ee236c.html</a>) at UCLA' can be found at <a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/proxgrad.pdf">http://www.seas.ucla.edu/~vandenbe/236C/lectures/proxgrad.pdf</a>.)</p>

<h4 id="proximal-mapping">Proximal mapping</h4>

<p>The <strong>proximal mapping</strong> (or proximal operator) of a convex function \(h\) is</p>

<p>\[ \mathbf{prox}_h(x) = \mathop{argmin}_u ( h(u) + \frac{1}{2} |u - x|^2_2 )\]</p>

<p><strong>examples</strong></p>

<p><strong>1.</strong> \(h(x) = 0: \mathbf{prox}_h(x) = x\)</p>

<p><strong>2.</strong> \(h(x) = I_C(x)\) (indicator function of \(C\)): \(\mathbf{prox}_h\) is projection on \(C\)</p>

<p>\[ \mathbf{prox}<em>h(x) = P_C(x) = \mathop{argmin}</em>{u \in C} |u - x|^2_2 \]</p>

<p><strong>3.</strong> \(h(x) = t |x|_1\): \(\mathbf{prox}_h\) is shinkage (soft threshold) operation</p>

<p>\[ \mathbf{prox}_h = \begin{cases}
    x_i - t &amp; x_i   \geqslant t <br />
    0       &amp; |x_i| \leqslant t <br />
    x_i + t &amp; x_i   \leqslant -t
\end{cases} \]</p>

<h4 id="proximal-gradient-method-1">Proximal gradient method</h4>

<p><strong>unconstrained problem</strong> with cost function split in two components</p>

<p>\[ \mathop{argmin} f(x) = g(x) + h(x) \]</p>

<p>\(g\) convex, differentiable, with <strong>dom</strong> \(g=\Re^n\)</p>

<p>\(h\) closed, convex, possibly nondifferentiable; \(\mathbf{prox}_h\) is inexpensive</p>

<p><strong>proximal gradient algorithm</strong></p>

<p>\[ x^{(k)} = \mathbf{prox}_{t_kh} ( x^{(k-1)} - t_k \nabla g ( x^{(k-1)} ) ) \]</p>

<p>\[ t_k &gt; 0 \text{ is the step size,}\]</p>

<p>constant or determined by line search</p>

<h4 id="interpretation">Interpretation</h4>

<p>\[ x^+ = \mathbf{prox}_{th} ( x - t\nabla g(x) ) \]</p>

<p>from definition of proximal operator:</p>

<p>\[ \begin{eqnarray<em>}
x^+ &amp; = &amp;  \mathop{argmin}_u ( h(u) + \frac{1}{2t} | u - x + t\nabla g(x) |^2_2 ) <br />
    &amp; = &amp; \mathop{argmin}_u ( h(u) + g(x) + \nabla g(x)^T(u-x) + \frac{1}{2t} | u - x |^2_2 )
\end{eqnarray</em>}\]</p>

<p>\(x^+\) minimizes \(h(u)\) plus a simple quadratic local of \(g(u)\) around \(x\)</p>

<h4 id="examples">Examples</h4>

<p>\[ minimize \; \; g(x) + h(x) \]</p>

<p><strong>gradient method</strong>: \(h(x) = 0\), i.e., minimize g(x)</p>

<p>\[ x^{(k)} = x^{(k-1)} - t_k\nabla g( x^{(k-1)} )\]</p>

<p><strong>gradient projection method</strong>: \(h(x) = I_C(x)\), i.e., minimize \(g(x)\) over \(C\)</p>

<p>\[ x^{(k)} = P_C ( x^{(k-1)} - t_k\nabla g (x^{(k-1)} ) ) \]</p>

<p><strong>iterative soft-thresholding</strong>: \(h(x) = |x|_1\), i.e., \( minimize \; \; g(x)+ | x |_1\)</p>

<p>\[ x^{(k)} = \mathbf{prox}_{t_kh} ( x^{(k-1)} - t_k\nabla g( x^{(k-1)} )  ) \]</p>

<p>and</p>

<p>\[ \mathbf{prox}_{th}(u)_i = 
\begin{cases}
u_i - t &amp; &amp; u_i \geq t <br />
0       &amp; &amp; -t \leq u_i \leq t <br />
u_i + t &amp; &amp; u_i \geq t
\end{cases}\]</p>

<p><img src="/jekyll32/images/fukushima-softthresholding.jpg" alt="" /></p>

<h3 id="dual-proximal-gradient-methods">2. Dual Proximal Gradient Methods</h3>

<p>参考 L. Vandenberghe EE236C (Spring 2013-14)</p>

<h4 id="composite-structure-in-the-dual">Composite structure in the Dual</h4>

<p>\[ \begin{eqnarray<em>}
minimize &amp; &amp; f(x)+g(Ax) <br />
maximize &amp; &amp; -f^\ast ( -A^Tz ) - g^\ast(z)
\end{eqnarray</em>}\]</p>

<p>dual has the right structure for the proximal gradient method if</p>

<p>prox-operator of \(g\) (or \(g^\ast\)) is cheap (closed form or simple algorithm)</p>

<p>\(f\) is strongly convex (\(f(x)-(\frac{\mu}{2})x^T\) is convex) implies \(f^\ast(-A^Tz)\) has Lipschitz continuous gradient (\(L=\frac{|A|^2_2}{\mu}\)):</p>

<p>\[ | A\nabla f^\ast(-A^Tu)-A\nabla f^\ast(-A^Tv) |_2 \leq \frac{|A|^2_2}{\mu}|u-v|_2 \]</p>

<p>because \(\nabla f^2\) is Lipschitz continuous with constant \(\frac{1}{\mu}\)</p>

<h4 id="dual-proximal-gradient-update">Dual proximal gradient update</h4>

<p>\[ z^+ = prox_{tg\ast}( z+tA\nabla f^\ast( -A^Tz ) ) \]</p>

<p>equivalent expression in term of \(f\):</p>

<p>\[ z^+ = prox_{tg\ast}(z+tA\hat{x}) \text{  where } \hat{x} = \mathop{argmin}_x ( f(x) + z^TAx )\]</p>

<p><strong>1.</strong>  if \(f\) is separable, calculation of \(\hat{x}\) decomposes into independent problems</p>

<p><strong>2.</strong>  step size \(t\) constant or from backtracking line search</p>

<h4 id="alternating-minimization-interpretation">Alternating minimization interpretation</h4>

<p>Moreau decomposition gives alternate expression for \(z\)-update</p>

<p>\[ z^+ = z + t(A\hat{x} - \hat{y}) \]</p>

<p>where</p>

<p>\[ \begin{eqnarray<em>}
\hat{x} &amp; = &amp; \mathop{argmin}_x ( f(x) + z^TAx ) <br />
\hat{y} &amp; = &amp; prox_{t^{-1}g} ( \frac{z}{t} + A\hat{x} )        <br />
        &amp; = &amp; \mathop{argmin}_y (g(y) + z^T(A\hat{x} - y) + \frac{t}{2} |A\hat{x} - y|^2_2  )
\end{eqnarray</em>}\]</p>

<p>in each iteration, an alternating minimization of:</p>

<p><strong>1. Lagrangian</strong> \(f(x) + g(y) + z^T(Ax - y)\) over \(x\)</p>

<p><strong>2. augmented Lagrangian</strong> \(f(x) + g(y) + z^T(Ax - y) + \frac{t}{2} |Ax - y|^2_2\) over \(y\)</p>

<h4 id="regularized-norm-approximation">Regularized norm approximation</h4>

<p>\[ minimize f(x) + |Ax - b| \text{   (with } f \text{ strongly convex)   } \]</p>

<p>a special case with \(g(y) = |y - b|\)</p>

<p>\[
g^\ast = \begin{cases}
b^Tz    &amp; &amp; |z|_\ast \leq 1 <br />
+\infty &amp; &amp; otherwise 
\end{cases}
\]</p>

<p>\[
prox_{tg\ast}(z) = P_C(z - tb)
\]</p>

<p>C is unit norm ball for dual norm \(|\cdot|_\ast\)</p>

<p><strong>dual gradient projection update</strong></p>

<p>\[ \begin{eqnarray<em>}
\hat{x} &amp; = &amp; \mathop{argmin}_x ( f(x) + z^TAx ) <br />
z^+     &amp; = &amp; P_C(z + t(A\hat{x} - b))
\end{eqnarray</em>}\]</p>

<h4 id="example">Example</h4>

<p>\[
minimize \; \; f(x) + \sum^p_{i=1}|B_ix|_2 \text{   (with } f \text{ strongly convex)   }
\]</p>

<p><strong>dual gradient projection update</strong></p>

<p>\[ \begin{eqnarray<em>}
\hat{x} &amp; = &amp; \mathop{argmin}_x ( f(x) + (\sum^p_{i=1}B^T_iz_i)^Tx ) <br />
z^+_i   &amp; = &amp; P_{C_i}(z_i + tB_i\hat{x}) \text{, } \; \; i=1, \cdots, p
\end{eqnarray</em>}\]</p>

<p>\(C_i\) is unit Euclidean norm ball in \(\Re^{m_i}\), if \(B_i \in \Re^{m_i \times n}\)</p>

<h4 id="minimization-over-intersection-of-convex-sets">Minimization over intersection of convex sets</h4>

<p>\[ \begin{eqnarray<em>}
minimize   &amp; &amp; f(x) <br />
subject to &amp; &amp; x \in C_i \cap \cdots \cap C_m
\end{eqnarray</em>}\]</p>

<p>\(f\) strongly convex; e.g., \(f(x) = |x - a|^2_2\) for projecting \(a\) on intersection</p>

<p>sets \(C_i\) are closed, convex, and easy to project onto</p>

<p><strong>dual proximal gradient update</strong></p>

<p>\[ \begin{eqnarray<em>}
\hat{x} &amp; = &amp; \mathop{argmin}_x ( f(x) + (z_i + \cdots + z_m)^Tx ) <br />
z^+_i   &amp; = &amp; z_i + t\hat{x} - tP_{C_i}(\frac{z_i}{t} + \hat{x}) \text{, }\; \; i=1, \cdots, m
\end{eqnarray</em>}\]</p>

<h4 id="decomposition-of-separable-problems">Decomposition of separable problems</h4>

<p>\[
minimize \; \; \sum^n_{j=1}f_j(x_j) + \sum^m_{i=1}g_i(A_{i1}x_1 + \cdots + A_{in}x_n )
\]</p>

<p>each \(f_i\) is strongly convex; \(g_i\) has inexpensive prox-operator</p>

<p><strong>dual proximal gradient update</strong></p>

<p>\[ \begin{eqnarray<em>}
\hat{x}_j &amp; = &amp; \mathop{argmin}_{x_j} ( f_j(x_j) + \sum^m_{i=1}z^T_iA_{ij}x_j ) \text{, } \; \; j=1, \cdots, n <br />
z^+_i        &amp; = &amp; prox_{tg^\ast_i}(z_i + t\sum^n_{j=1}A_{ij}\hat{x}_j ) \text{, } \; \; i=1, \cdots, m
\end{eqnarray</em>}\]</p>

<h3 id="fast-proximal-gradient-methods">3. Fast proximal gradient methods</h3>

<p>参考 L. Vandenberghe EE236C (Spring 2013-14)</p>

<h4 id="fista-basic-version">FISTA (basic version)</h4>

<p>\[
minimize \; \; f(x) = g(x) + h(x)
\]</p>

<p>\(g\) convex, differentiable with \(\mathop{dom} g=\Re^n\)</p>

<p>\(h\) closed, convex, with inexpensive \(prox_{th}\) operator</p>

<p><strong>algorithm</strong>: choose any \(x^{(0)} = x^{(-1)}\); for \(k \geq 1\), repeat the steps</p>

<p>\[ \begin{eqnarray<em>}
y             &amp; = &amp; x^{(k-1)} + \frac{k-2}{k+1} ( x^{(k-1)} - x^{(k-2)} ) <br />
x^{(k)} &amp; = &amp; prox_{t_kh} ( y - t_k\nabla g(y) )
\end{eqnarray</em>}\]</p>

<p>step size \(t_k\) fixed or determined by line search</p>

<p>acronym stands for 'Fast Iterative Shrinkage-Thresholding Algorithm'</p>

<h4 id="interpretation-1">Interpretation</h4>

<p>first iteration (\(k = 1\)) is a proximal gradient step at \(y = x^{(0)}\)</p>

<p>next iterations are proximal gradient steps at extrapolated points \(y\)</p>

<p><img src="/jekyll32/images/fukushima-interpretation.png" alt="" /></p>

<p>note: \(x^{(k)}\) is feasible (in \(\mathop{dom} h\)); \(y\) may be outside \(\mathop{dom} h\)</p>

<h4 id="reformulation-of-fista">Reformulation of FISTA</h4>

<p>define \(\theta_k = \frac{2}{k+1}\) and introduce an intermediate variable \(v^{(k)}\)</p>

<p><strong>algorithm</strong>: choose \(x^{(0)} = v^{(0)}\); for \(k \geq 1\), repeat the steps</p>

<p>\[ \begin{eqnarray<em>}
y       &amp; = &amp; (1 - \theta_k)x^{(k-1)} + \theta_kv^{(k-1)} <br />
x^{(k)} &amp; = &amp; prox_{t_kh}(y-t_k\nabla g(y))<br />
v^{(k)} &amp; = &amp; x^{(k - 1)} + \frac{1}{\theta_k}( x^{(k)} - x^{(k-1)} )
\end{eqnarray</em>}\]</p>

<h4 id="nesterovs-second-method">Nesterov's second method</h4>

<p><strong>algorithm</strong>: choose \(x^{(0)} = v^{(0)}\); for \(k \geq 1\), repeat the steps</p>

<p>\[ \begin{eqnarray<em>}
y             &amp; = &amp; (1 - \theta_k)x^{(k-1)} + \theta_kv^{(k-1)} <br />
v^{(k)} &amp; = &amp; prox_{(\frac{t_k}{\theta_k})h} ( v^{(k-1)} - \frac{t_k}{\theta_k}\nabla g(y) )<br />
x^{(k)} &amp; = &amp; (1 - \theta_k)x^{(k-1)} + \theta_kv^{(k)}
\end{eqnarray</em>}\]</p>

<p>User\(\theta_k = \frac{2}{k+1}\) and \(t_k = \frac{1}{L}\), or one of the line search methods</p>

<p>identical to FISTA if \(h(x) = 0\)</p>

<p>unlike in FISTA, \(y\) is feasible (in \(\mathop{dom} h\)) if we take \(x^{(0)} \in \mathop{dom} h\)</p>

<h3 id="fast-dual-proximal-gradient-methods">4. Fast dual proximal gradient methods</h3>

<p>参考 A Fast Dual Proximal Gradient Algorithm for Convex Minimization and Applications by Amir Beck and Marc Teboulle at October 10, 2013</p>

<p>\[ \begin{eqnarray<em>}
(D)   &amp; = &amp; \max_y\lbrace q(y) \equiv -f^\ast(A^Ty)-g^\ast(-y)\rbrace,<br />
(D') &amp; = &amp; \min F(y) + G(y),<br />
(P') &amp; = &amp; \min \lbrace f(x) + g(z): Ax - z = 0 \rbrace.
\end{eqnarray</em>}\]</p>

<p>\[
F(y) := f^\ast( A^Ty ), \; \; G(y) :=g^\ast(-y)
\]</p>

<p>Initialization: \(L \geq \frac{|A|^2}{\sigma}\), \(w_1 = y_0 \in \mathbb{V}\), \(t_1 = 1\).</p>

<p>General Step \((k \geq 1)\):</p>

<p>\[ \begin{eqnarray<em>}
y_k           &amp; = &amp; prox_{\frac{1}{L}G}( w_k - \frac{1}{L} \nabla F(w_k) )<br />
t_{k+1}   &amp; = &amp; \frac{1 + \sqrt{1 + 4t^2_k}}{2} <br />
w_{k+1} &amp; = &amp; y_k + ( \frac{t_k - 1}{t_{k+1}} ) (y_k - y_{k-1}).
\end{eqnarray</em>}\]</p>

<h4 id="the-fast-dual-based-proximal-gradient-method-fdpg">The Fast Dual-Based Proximal Gradient Method (FDPG)</h4>

<p>Input: \(L \geq \frac{|A|^2}{\sigma} - \text{ an upper bound on the Lipschitz constant of } \nabla F\)</p>

<p>Step \(0\). Take \(w_1 = y_0 \in \mathbb{V}\), \(t_1 = 1\).</p>

<p>Step \(k\). (\(k \geq 0\)) Compute</p>

<p>\[ \begin{eqnarray<em>}
u_k           &amp; = &amp; \mathop{argmax}_x \lbrace &lt;x, A^Tw_k&gt; - f(x) \rbrace<br />
v_k           &amp; = &amp; prox_{Lg}(Au_k - Lw_k)<br />
y_k           &amp; = &amp; w_k - \frac{1}{L}(au_k - v_k)<br />
t_{k+1}   &amp; = &amp; \frac{1 + \sqrt{1 + 4t^2_k}}{2}<br />
w_{k+1} &amp; = &amp; y_k + ( \frac{t_k - 1}{t_{k+1}} ) (y_k - y_{k-1}). \tag</em>{$\blacksquare$}
\end{eqnarray*}\]</p>

  </div>

</article>

      </div>
    </div>

    

  </body>

</html>

